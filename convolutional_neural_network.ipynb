{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to train a neural network model in order to read hand-written digits automatically. It uses the `Tensorflow` library, developed by Google.\n",
    "\n",
    "Although the notebook is divided into smaller steps, three main task will be of interest: network conception, optimization design and model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: module imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among necessary modules, there is of course Tensorflow; but also an utilitary for reading state-of-the-art data sets, like MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "# Alternative choice: from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: data recovering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Read in data using TF Learn's built in function to load MNIST data to the folder data/mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = read_data_sets(\"data\", one_hot=True, reshape=False, validation_size=0)\n",
    "# If alternative module import: mnist = input_data.read_data_sets(\"/data/mnist\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: parameter definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define paramaters for the model:\n",
    "- hidden layer depth (number of channel per convolutional and fully connected layer)\n",
    "- number of output classes\n",
    "- number of images per batch\n",
    "- number of epochs (one epoch = all images have been used for training)\n",
    "- decaying learning rate: fit the learning rate during training according to the convergence step (larger at the beginning, smaller at the end), the used formula is the following: min_lr + (max_lr-min_lr)*math.exp(-i/decay_speed), with i being the training iteration\n",
    "- dropout, *i.e.* percentage of nodes that are briefly removed during training process\n",
    "- printing frequency during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L_C1 = 32\n",
    "L_C2 = 64\n",
    "L_FC = 512\n",
    "N_CLASSES = 10\n",
    "\n",
    "BATCH_SIZE = 150\n",
    "N_EPOCHS = 5\n",
    "\n",
    "MAX_LR = 0.003\n",
    "MIN_LR = 0.0001\n",
    "DECAY_SPEED = 1000.0\n",
    "DROPOUT = 0.75\n",
    "\n",
    "SKIP_STEP = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: create placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Tensorflow, placeholders refer to variables that will be fed each time the model is run.\n",
    "\n",
    "Each image in the MNIST data is of shape 28*28*1 (greyscale) therefore, each image is represented with a 28*28*1 tensor; use None for shape so we can change the batch_size once we've built the tensor graph. The resulting output is a vector of `N_CLASSES` 0-1 values, the only '1' being the model prediction.\n",
    "\n",
    "As we work with a decaying learning rate, this quantity is managed within a placeholder. We'll be doing dropout for hidden layer so we'll need a placeholder for the dropout probability too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"data\"):\n",
    "    # input X: 28x28 grayscale images, the first dimension (None) will index the images in the mini-batch\n",
    "    X = tf.placeholder(tf.float32, [None, 28, 28, 1], name='X')\n",
    "    # If alternative module import: X = tf.placeholder(tf.float32, [None, 784], name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32, [None, N_CLASSES], name='Y')\n",
    "# variable learning rate\n",
    "lrate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "# dropout proportion\n",
    "dropout = tf.placeholder(tf.float32, name='dropout')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is composed of the following steps:\n",
    "\n",
    "conv -> relu -> pool -> conv -> relu -> pool -> fully connected -> softmax\n",
    "\n",
    "- conv: convolution between an input neuron and an image filter\n",
    "- relu (REctified Linear Unit): neuron activation function\n",
    "- pool: max pooling layer, that considers the maximal value in a n*n patch\n",
    "- fully connected: full connection between two consecutive neuron layer, concretized by a matrix multiplication\n",
    "- softmax: neuron activation function, associated with output\n",
    "\n",
    "They represent its structure, and may be showed within graph with `tensorboard` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('conv1') as scope:\n",
    "    # if alternative module import, reshape the image to [BATCH_SIZE, 28, 28, 1]\n",
    "    # X = tf.reshape(X, shape=[-1, 28, 28, 1])\n",
    "    # create kernel variable of dimension [5, 5, 1, 32]\n",
    "    kernel = tf.get_variable('kernel',\n",
    "                             [5, 5, 1, L_C1],\n",
    "                             initializer=tf.truncated_normal_initializer())\n",
    "    # create biases variable of dimension [32]\n",
    "    biases = tf.get_variable('biases',\n",
    "                             [L_C1],\n",
    "                             initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    # apply tf.nn.conv2d. strides [1, 1, 1, 1], padding is 'SAME'\n",
    "    conv = tf.nn.conv2d(X, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    # apply relu on the sum of convolution output and biases\n",
    "    conv1 = tf.nn.relu(conv+biases, name=scope.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output is of dimension BATCH_SIZE \\* 28 \\* 28 \\* 32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('pool1') as scope:\n",
    "    # apply max pool with ksize [1, 2, 2, 1], and strides [1, 2, 2, 1], padding 'SAME'    \n",
    "    pool1 = tf.nn.max_pool(conv1,\n",
    "                           ksize=[1, 2, 2, 1],\n",
    "                           strides=[1, 2, 2, 1],\n",
    "                           padding='SAME')\n",
    "\n",
    "    # output is of dimension BATCH_SIZE x 14 x 14 x 32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('conv2') as scope:\n",
    "    # similar to conv1, except kernel now is of the size 5 x 5 x 32 x 64\n",
    "    kernel = tf.get_variable('kernels', [5, 5, L_C1, L_C2], \n",
    "                        initializer=tf.truncated_normal_initializer())\n",
    "    biases = tf.get_variable('biases', [L_C2],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "    conv = tf.nn.conv2d(pool1, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv + biases, name=scope.name)\n",
    "\n",
    "    # output is of dimension BATCH_SIZE x 14 x 14 x 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('pool2') as scope:\n",
    "    # similar to pool1\n",
    "    pool2 = tf.nn.max_pool(conv2,\n",
    "                           ksize=[1, 2, 2, 1],\n",
    "                           strides=[1, 2, 2, 1],\n",
    "                           padding='SAME')\n",
    "\n",
    "    # output is of dimension BATCH_SIZE x 7 x 7 x 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('fc') as scope:\n",
    "    # use weight of dimension 7 * 7 * 64 x 1024\n",
    "    input_features = 7 * 7 * L_C2\n",
    "    \n",
    "    # create weights and biases\n",
    "    w = tf.get_variable('weights', [input_features, L_FC],\n",
    "                        initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('biases', [L_FC],\n",
    "                        initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    # reshape pool2 to 2 dimensional\n",
    "    pool2 = tf.reshape(pool2, [-1, input_features])\n",
    "\n",
    "    # apply relu on matmul of pool2 and w + b\n",
    "    fc = tf.nn.relu(tf.matmul(pool2, w) + b, name='relu')\n",
    "    \n",
    "    # apply dropout\n",
    "    fc = tf.nn.dropout(fc, dropout, name='relu_dropout')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('softmax_linear') as scope:\n",
    "    # get logits without softmax you need to create weights and biases\n",
    "    w = tf.get_variable('weights', [L_FC, N_CLASSES],\n",
    "                        initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('biases', [N_CLASSES],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "    logits = tf.matmul(fc, w) + b\n",
    "    Ypredict = tf.nn.softmax(logits)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: loss function design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use cross-entropy loss function (-sum(Y_i * log(Yi)) ), normalised for batches of 100 images.\n",
    "\n",
    "TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability problems with log(0) (which is NaN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    # cross-entropy between predicted and real values    \n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits)\n",
    "    loss = tf.reduce_mean(entropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    # accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Ypredict, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Define training optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Adam optimizer with decaying learning rate to minimize cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(lrate).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final step: running the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 21808.7, accuracy = 0.120\n",
      "Step 10: loss = 1881.9, accuracy = 0.713\n",
      "Step 20: loss = 1888.2, accuracy = 0.740\n",
      "Step 30: loss = 841.2, accuracy = 0.860\n",
      "Step 40: loss = 1039.0, accuracy = 0.807\n",
      "Step 50: loss = 536.4, accuracy = 0.907\n",
      "Step 60: loss = 223.1, accuracy = 0.920\n",
      "Step 70: loss = 872.0, accuracy = 0.840\n",
      "Step 80: loss = 212.1, accuracy = 0.933\n",
      "Step 90: loss = 283.3, accuracy = 0.927\n",
      "Step 100: loss = 480.6, accuracy = 0.907\n",
      "Step 110: loss = 302.5, accuracy = 0.927\n",
      "Step 120: loss = 317.8, accuracy = 0.893\n",
      "Step 130: loss = 433.6, accuracy = 0.920\n",
      "Step 140: loss =  80.2, accuracy = 0.967\n",
      "Step 150: loss = 155.1, accuracy = 0.953\n",
      "Step 160: loss = 253.0, accuracy = 0.933\n",
      "Step 170: loss = 242.0, accuracy = 0.927\n",
      "Step 180: loss =  94.2, accuracy = 0.980\n",
      "Step 190: loss =  95.8, accuracy = 0.940\n",
      "Step 200: loss = 105.0, accuracy = 0.953\n",
      "Step 210: loss = 167.0, accuracy = 0.940\n",
      "Step 220: loss = 138.4, accuracy = 0.947\n",
      "Step 230: loss =  96.2, accuracy = 0.953\n",
      "Step 240: loss = 191.6, accuracy = 0.933\n",
      "Step 250: loss = 218.6, accuracy = 0.933\n",
      "Step 260: loss = 120.5, accuracy = 0.940\n",
      "Step 270: loss = 412.0, accuracy = 0.913\n",
      "Step 280: loss = 147.8, accuracy = 0.933\n",
      "Step 290: loss = 153.1, accuracy = 0.947\n",
      "Step 300: loss = 242.7, accuracy = 0.947\n",
      "Step 310: loss = 209.9, accuracy = 0.927\n",
      "Step 320: loss = 175.9, accuracy = 0.920\n",
      "Step 330: loss =  22.1, accuracy = 0.987\n",
      "Step 340: loss =  80.7, accuracy = 0.953\n",
      "Step 350: loss = 149.7, accuracy = 0.953\n",
      "Step 360: loss =  65.9, accuracy = 0.960\n",
      "Step 370: loss =  86.2, accuracy = 0.920\n",
      "Step 380: loss =  75.6, accuracy = 0.953\n",
      "Step 390: loss = 103.8, accuracy = 0.933\n",
      "Step 400: loss =  25.5, accuracy = 0.980\n",
      "Step 410: loss =  70.8, accuracy = 0.947\n",
      "Step 420: loss =  41.0, accuracy = 0.973\n",
      "Step 430: loss = 102.9, accuracy = 0.960\n",
      "Step 440: loss =  11.6, accuracy = 0.980\n",
      "Step 450: loss = 131.9, accuracy = 0.953\n",
      "Step 460: loss = 146.9, accuracy = 0.960\n",
      "Step 470: loss =  48.5, accuracy = 0.973\n",
      "Step 480: loss =  71.7, accuracy = 0.953\n",
      "Step 490: loss =  54.7, accuracy = 0.993\n",
      "Step 500: loss =  45.7, accuracy = 0.960\n",
      "Step 510: loss =  26.4, accuracy = 0.980\n",
      "Step 520: loss =  31.9, accuracy = 0.973\n",
      "Step 530: loss = 132.3, accuracy = 0.940\n",
      "Step 540: loss =  38.6, accuracy = 0.960\n",
      "Step 550: loss =  21.5, accuracy = 0.967\n",
      "Step 560: loss =  34.8, accuracy = 0.973\n",
      "Step 570: loss =  61.2, accuracy = 0.987\n",
      "Step 580: loss =  30.5, accuracy = 0.967\n",
      "Step 590: loss = 116.9, accuracy = 0.960\n",
      "Step 600: loss =  25.4, accuracy = 0.980\n",
      "Step 610: loss =  12.6, accuracy = 0.987\n",
      "Step 620: loss =  61.4, accuracy = 0.960\n",
      "Step 630: loss =  70.0, accuracy = 0.967\n",
      "Step 640: loss =  35.9, accuracy = 0.947\n",
      "Step 650: loss =  43.5, accuracy = 0.960\n",
      "Step 660: loss =  62.6, accuracy = 0.953\n",
      "Step 670: loss = 116.2, accuracy = 0.940\n",
      "Step 680: loss =  31.1, accuracy = 0.973\n",
      "Step 690: loss =  37.7, accuracy = 0.967\n",
      "Step 700: loss =  38.9, accuracy = 0.973\n",
      "Step 710: loss =  55.4, accuracy = 0.967\n",
      "Step 720: loss =  40.0, accuracy = 0.960\n",
      "Step 730: loss =  65.5, accuracy = 0.967\n",
      "Step 740: loss =  57.7, accuracy = 0.973\n",
      "Step 750: loss =  16.3, accuracy = 0.980\n",
      "Step 760: loss =  12.0, accuracy = 0.987\n",
      "Step 770: loss =  20.6, accuracy = 0.987\n",
      "Step 780: loss =  31.0, accuracy = 0.993\n",
      "Step 790: loss =  30.2, accuracy = 0.980\n",
      "Step 800: loss =  63.4, accuracy = 0.947\n",
      "Step 810: loss =  22.0, accuracy = 0.973\n",
      "Step 820: loss =   3.4, accuracy = 0.993\n",
      "Step 830: loss =  38.8, accuracy = 0.967\n",
      "Step 840: loss =  28.1, accuracy = 0.980\n",
      "Step 850: loss =  34.5, accuracy = 0.960\n",
      "Step 860: loss =  15.4, accuracy = 0.993\n",
      "Step 870: loss =  28.4, accuracy = 0.967\n",
      "Step 880: loss =  36.8, accuracy = 0.960\n",
      "Step 890: loss =   1.3, accuracy = 0.993\n",
      "Step 900: loss =  28.3, accuracy = 0.967\n",
      "Step 910: loss =  26.7, accuracy = 0.967\n",
      "Step 920: loss =  25.3, accuracy = 0.960\n",
      "Step 930: loss =  23.6, accuracy = 0.980\n",
      "Step 940: loss =   0.0, accuracy = 1.000\n",
      "Step 950: loss =  26.9, accuracy = 0.973\n",
      "Step 960: loss =  26.9, accuracy = 0.973\n",
      "Step 970: loss =  26.1, accuracy = 0.980\n",
      "Step 980: loss =  60.3, accuracy = 0.953\n",
      "Step 990: loss =   1.4, accuracy = 0.987\n",
      "Step 1000: loss =  72.1, accuracy = 0.967\n",
      "Step 1010: loss =  45.1, accuracy = 0.960\n",
      "Step 1020: loss =  25.1, accuracy = 0.967\n",
      "Step 1030: loss =  34.2, accuracy = 0.967\n",
      "Step 1040: loss =  44.7, accuracy = 0.987\n",
      "Step 1050: loss =  35.9, accuracy = 0.980\n",
      "Step 1060: loss =  45.2, accuracy = 0.973\n",
      "Step 1070: loss =  19.6, accuracy = 0.980\n",
      "Step 1080: loss =  18.3, accuracy = 0.987\n",
      "Step 1090: loss =  16.0, accuracy = 0.980\n",
      "Step 1100: loss =  44.3, accuracy = 0.973\n",
      "Step 1110: loss =  10.2, accuracy = 0.987\n",
      "Step 1120: loss =  12.2, accuracy = 0.993\n",
      "Step 1130: loss =  19.9, accuracy = 0.973\n",
      "Step 1140: loss =   5.7, accuracy = 0.980\n",
      "Step 1150: loss =   8.9, accuracy = 0.987\n",
      "Step 1160: loss =  53.6, accuracy = 0.967\n",
      "Step 1170: loss =  21.9, accuracy = 0.980\n",
      "Step 1180: loss =  18.0, accuracy = 0.980\n",
      "Step 1190: loss =  45.9, accuracy = 0.960\n",
      "Step 1200: loss =   2.9, accuracy = 0.993\n",
      "Step 1210: loss =  25.0, accuracy = 0.967\n",
      "Step 1220: loss =   0.0, accuracy = 1.000\n",
      "Step 1230: loss =  16.3, accuracy = 0.980\n",
      "Step 1240: loss =  56.0, accuracy = 0.960\n",
      "Step 1250: loss =  16.4, accuracy = 0.960\n",
      "Step 1260: loss =  52.3, accuracy = 0.960\n",
      "Step 1270: loss =  18.5, accuracy = 0.980\n",
      "Step 1280: loss =  25.2, accuracy = 0.993\n",
      "Step 1290: loss =  12.8, accuracy = 0.980\n",
      "Step 1300: loss =   4.9, accuracy = 0.987\n",
      "Step 1310: loss =   6.1, accuracy = 0.993\n",
      "Step 1320: loss =   7.2, accuracy = 0.980\n",
      "Step 1330: loss =  24.6, accuracy = 0.953\n",
      "Step 1340: loss =  12.9, accuracy = 0.987\n",
      "Step 1350: loss =  34.4, accuracy = 0.967\n",
      "Step 1360: loss =   5.9, accuracy = 0.980\n",
      "Step 1370: loss =  19.6, accuracy = 0.980\n",
      "Step 1380: loss =   9.7, accuracy = 0.987\n",
      "Step 1390: loss =  28.3, accuracy = 0.993\n",
      "Step 1400: loss =  44.0, accuracy = 0.960\n",
      "Step 1410: loss =  26.8, accuracy = 0.980\n",
      "Step 1420: loss =  18.2, accuracy = 0.973\n",
      "Step 1430: loss =  11.2, accuracy = 0.987\n",
      "Step 1440: loss =  12.6, accuracy = 0.993\n",
      "Step 1450: loss =   8.1, accuracy = 0.987\n",
      "Step 1460: loss =   0.0, accuracy = 1.000\n",
      "Step 1470: loss =  20.0, accuracy = 0.993\n",
      "Step 1480: loss =   6.7, accuracy = 0.987\n",
      "Step 1490: loss =  17.9, accuracy = 0.980\n",
      "Step 1500: loss =   5.5, accuracy = 0.993\n",
      "Step 1510: loss =  10.3, accuracy = 0.987\n",
      "Step 1520: loss =  54.8, accuracy = 0.967\n",
      "Step 1530: loss =  10.4, accuracy = 0.993\n",
      "Step 1540: loss =  47.0, accuracy = 0.973\n",
      "Step 1550: loss =   4.3, accuracy = 0.993\n",
      "Step 1560: loss =   7.9, accuracy = 0.980\n",
      "Step 1570: loss =  13.6, accuracy = 0.980\n",
      "Step 1580: loss =   0.0, accuracy = 1.000\n",
      "Step 1590: loss =  20.0, accuracy = 0.967\n",
      "Step 1600: loss =   0.0, accuracy = 1.000\n",
      "Step 1610: loss =  18.6, accuracy = 0.973\n",
      "Step 1620: loss =   5.8, accuracy = 0.993\n",
      "Step 1630: loss =  25.6, accuracy = 0.980\n",
      "Step 1640: loss =  12.3, accuracy = 0.980\n",
      "Step 1650: loss =  19.8, accuracy = 0.973\n",
      "Step 1660: loss =   7.4, accuracy = 0.987\n",
      "Step 1670: loss =   8.1, accuracy = 0.993\n",
      "Step 1680: loss =   0.2, accuracy = 0.993\n",
      "Step 1690: loss =  32.8, accuracy = 0.980\n",
      "Step 1700: loss =  11.6, accuracy = 0.987\n",
      "Step 1710: loss =  12.1, accuracy = 0.973\n",
      "Step 1720: loss =  23.2, accuracy = 0.967\n",
      "Step 1730: loss =   0.0, accuracy = 1.000\n",
      "Step 1740: loss =   4.6, accuracy = 0.980\n",
      "Step 1750: loss =   5.4, accuracy = 0.980\n",
      "Step 1760: loss =   0.9, accuracy = 0.993\n",
      "Step 1770: loss =   0.4, accuracy = 0.993\n",
      "Step 1780: loss =   5.4, accuracy = 0.980\n",
      "Step 1790: loss =   7.1, accuracy = 0.980\n",
      "Step 1800: loss =   5.7, accuracy = 0.993\n",
      "Step 1810: loss =   1.2, accuracy = 0.993\n",
      "Step 1820: loss =  18.5, accuracy = 0.973\n",
      "Step 1830: loss =  11.3, accuracy = 0.980\n",
      "Step 1840: loss =  26.6, accuracy = 0.973\n",
      "Step 1850: loss =   0.0, accuracy = 1.000\n",
      "Step 1860: loss =  16.6, accuracy = 0.973\n",
      "Step 1870: loss =  17.7, accuracy = 0.973\n",
      "Step 1880: loss =  43.8, accuracy = 0.980\n",
      "Step 1890: loss =  14.5, accuracy = 0.987\n",
      "Step 1900: loss =  43.2, accuracy = 0.973\n",
      "Step 1910: loss =   9.9, accuracy = 0.973\n",
      "Step 1920: loss =   8.8, accuracy = 0.987\n",
      "Step 1930: loss =  17.8, accuracy = 0.973\n",
      "Step 1940: loss =   2.2, accuracy = 0.993\n",
      "Step 1950: loss =   0.0, accuracy = 1.000\n",
      "Step 1960: loss =  27.7, accuracy = 0.967\n",
      "Step 1970: loss =   4.8, accuracy = 0.987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1980: loss =   0.0, accuracy = 1.000\n",
      "Step 1990: loss =  39.3, accuracy = 0.967\n",
      "Optimization Finished!\n",
      "Total time: 901.09 seconds\n",
      "Accuracy = 0.956; loss = 45.337\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # to visualize using TensorBoard (tensorboard --logdir=\"./graphs/convnet\" --port 6006)\n",
    "    writer = tf.summary.FileWriter('./graphs/convnet', sess.graph)\n",
    "\n",
    "    start_time = time.time()\n",
    "    n_batches = int(mnist.train.num_examples / BATCH_SIZE)\n",
    "\n",
    "    # Train the model\n",
    "    for index in range(n_batches * N_EPOCHS): # train the model n_epochs times\n",
    "        X_batch, Y_batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "        learning_rate = MIN_LR + (MAX_LR - MIN_LR) * math.exp(-index/DECAY_SPEED)\n",
    "        \n",
    "        if index % SKIP_STEP == 0:\n",
    "            loss_batch, accuracy_batch = sess.run([loss, accuracy], \n",
    "                                feed_dict={X: X_batch, Y:Y_batch, lrate: learning_rate, dropout: 1.0}) \n",
    "            print('Step {}: loss = {:5.1f}, accuracy = {:1.3f}'.format(index, loss_batch, accuracy_batch))\n",
    "    \n",
    "        sess.run(optimizer, feed_dict={X: X_batch, Y: Y_batch, lrate: learning_rate, dropout: DROPOUT})\n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time: {:.2f} seconds\".format(time.time() - start_time))\n",
    "    \n",
    "    # Test the model\n",
    "    X_batch, Y_batch = mnist.test.next_batch(BATCH_SIZE)\n",
    "    _, loss_batch, accuracy_batch = sess.run([optimizer, loss, accuracy], \n",
    "                                    feed_dict={X: mnist.test.images, Y: mnist.test.labels, lrate: learning_rate, dropout: DROPOUT}) \n",
    "    print(\"Accuracy = {:1.3f}; loss = {:1.3f}\".format(accuracy_batch, loss_batch))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
