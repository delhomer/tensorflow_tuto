{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to train a neural network model in order to read hand-written digits automatically. It uses the `Tensorflow` library, developed by Google.\n",
    "\n",
    "Although the notebook is divided into smaller steps, three main task will be of interest: network conception, optimization design and model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: module imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among necessary modules, there is of course Tensorflow; but also an utilitary for reading state-of-the-art data sets, like MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "# Alternative choice: from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: data recovering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Read in data using TF Learn's built in function to load MNIST data to the folder data/mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = read_data_sets(\"data\", one_hot=True, reshape=False, validation_size=0)\n",
    "# If alternative module import: mnist = input_data.read_data_sets(\"/data/mnist\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: parameter definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define paramaters for the model:\n",
    "- hidden layer depth (number of channel per convolutional and fully connected layer)\n",
    "- number of output classes\n",
    "- number of images per batch\n",
    "- number of epochs (one epoch = all images have been used for training)\n",
    "- decaying learning rate: fit the learning rate during training according to the convergence step (larger at the beginning, smaller at the end), the used formula is the following: min_lr + (max_lr-min_lr)*math.exp(-i/decay_speed), with i being the training iteration\n",
    "- dropout, *i.e.* percentage of nodes that are briefly removed during training process\n",
    "- printing frequency during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L_C1 = 32\n",
    "L_C2 = 64\n",
    "L_FC = 512\n",
    "N_CLASSES = 10\n",
    "\n",
    "BATCH_SIZE = 150\n",
    "N_EPOCHS = 5\n",
    "\n",
    "MAX_LR = 0.003\n",
    "MIN_LR = 0.0001\n",
    "DECAY_SPEED = 1000.0\n",
    "DROPOUT = 0.75\n",
    "\n",
    "SKIP_STEP = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: create placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Tensorflow, placeholders refer to variables that will be fed each time the model is run.\n",
    "\n",
    "Each image in the MNIST data is of shape 28\\*28\\*1 (greyscale) therefore, each image is represented with a 28\\*28\\*1 tensor; use None for shape so we can change the batch_size once we've built the tensor graph. The resulting output is a vector of `N_CLASSES` 0-1 values, the only '1' being the model prediction.\n",
    "\n",
    "As we work with a decaying learning rate, this quantity is managed within a placeholder. We'll be doing dropout for hidden layer so we'll need a placeholder for the dropout probability too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"data\"):\n",
    "    # input X: 28x28 grayscale images, the first dimension (None) will index the images in the mini-batch\n",
    "    X = tf.placeholder(tf.float32, [None, 28, 28, 1], name='X')\n",
    "    # If alternative module import: X = tf.placeholder(tf.float32, [None, 784], name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32, [None, N_CLASSES], name='Y')\n",
    "# variable learning rate\n",
    "lrate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "# dropout proportion\n",
    "dropout = tf.placeholder(tf.float32, name='dropout')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is composed of the following steps:\n",
    "\n",
    "conv -> relu -> pool -> conv -> relu -> pool -> fully connected -> softmax\n",
    "\n",
    "- conv: convolution between an input neuron and an image filter\n",
    "- relu (REctified Linear Unit): neuron activation function\n",
    "- pool: max pooling layer, that considers the maximal value in a n*n patch\n",
    "- fully connected: full connection between two consecutive neuron layer, concretized by a matrix multiplication\n",
    "- softmax: neuron activation function, associated with output\n",
    "\n",
    "They represent its structure, and may be showed within graph with `tensorboard` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('conv1') as scope:\n",
    "    # if alternative module import, reshape the image to [BATCH_SIZE, 28, 28, 1]\n",
    "    # X = tf.reshape(X, shape=[-1, 28, 28, 1])\n",
    "    # create kernel variable of dimension [5, 5, 1, 32]\n",
    "    kernel = tf.get_variable('kernel',\n",
    "                             [5, 5, 1, L_C1],\n",
    "                             initializer=tf.truncated_normal_initializer())\n",
    "    # create biases variable of dimension [32]\n",
    "    biases = tf.get_variable('biases',\n",
    "                             [L_C1],\n",
    "                             initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    # apply tf.nn.conv2d. strides [1, 1, 1, 1], padding is 'SAME'\n",
    "    conv = tf.nn.conv2d(X, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    # apply relu on the sum of convolution output and biases\n",
    "    conv1 = tf.nn.relu(conv+biases, name=scope.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output is of dimension BATCH_SIZE \\* 28 \\* 28 \\* 32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('pool1') as scope:\n",
    "    # apply max pool with ksize [1, 2, 2, 1], and strides [1, 2, 2, 1], padding 'SAME'    \n",
    "    pool1 = tf.nn.max_pool(conv1,\n",
    "                           ksize=[1, 2, 2, 1],\n",
    "                           strides=[1, 2, 2, 1],\n",
    "                           padding='SAME')\n",
    "\n",
    "    # output is of dimension BATCH_SIZE x 14 x 14 x 32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('conv2') as scope:\n",
    "    # similar to conv1, except kernel now is of the size 5 x 5 x 32 x 64\n",
    "    kernel = tf.get_variable('kernels', [5, 5, L_C1, L_C2], \n",
    "                        initializer=tf.truncated_normal_initializer())\n",
    "    biases = tf.get_variable('biases', [L_C2],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "    conv = tf.nn.conv2d(pool1, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv + biases, name=scope.name)\n",
    "\n",
    "    # output is of dimension BATCH_SIZE x 14 x 14 x 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('pool2') as scope:\n",
    "    # similar to pool1\n",
    "    pool2 = tf.nn.max_pool(conv2,\n",
    "                           ksize=[1, 2, 2, 1],\n",
    "                           strides=[1, 2, 2, 1],\n",
    "                           padding='SAME')\n",
    "\n",
    "    # output is of dimension BATCH_SIZE x 7 x 7 x 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('fc') as scope:\n",
    "    # use weight of dimension 7 * 7 * 64 x 1024\n",
    "    input_features = 7 * 7 * L_C2\n",
    "    \n",
    "    # create weights and biases\n",
    "    w = tf.get_variable('weights', [input_features, L_FC],\n",
    "                        initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('biases', [L_FC],\n",
    "                        initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    # reshape pool2 to 2 dimensional\n",
    "    pool2 = tf.reshape(pool2, [-1, input_features])\n",
    "\n",
    "    # apply relu on matmul of pool2 and w + b\n",
    "    fc = tf.nn.relu(tf.matmul(pool2, w) + b, name='relu')\n",
    "    \n",
    "    # apply dropout\n",
    "    fc = tf.nn.dropout(fc, dropout, name='relu_dropout')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('softmax_linear') as scope:\n",
    "    # get logits without softmax you need to create weights and biases\n",
    "    w = tf.get_variable('weights', [L_FC, N_CLASSES],\n",
    "                        initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('biases', [N_CLASSES],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "    logits = tf.matmul(fc, w) + b\n",
    "    Ypredict = tf.nn.softmax(logits)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: loss function design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use cross-entropy loss function (-sum(Y_i * log(Yi)) ), normalised for batches of 100 images.\n",
    "\n",
    "TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability problems with log(0) (which is NaN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    # cross-entropy between predicted and real values    \n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits)\n",
    "    loss = tf.reduce_mean(entropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    # accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Ypredict, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Define training optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Adam optimizer with decaying learning rate to minimize cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(lrate).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final step: running the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 31084.2, accuracy = 0.087\n",
      "[[  9520.640625    24725.2265625  -13337.17871094 ...,  20074.74804688\n",
      "   12452.28320312 -28204.671875  ]\n",
      " [ 24203.11328125  14676.46191406  -6655.67822266 ...,   2485.44897461\n",
      "   16932.69140625 -24469.2890625 ]\n",
      " [ 35237.85546875  36522.23046875 -36162.12109375 ...,  15385.97265625\n",
      "   14692.63671875 -22220.6640625 ]\n",
      " ..., \n",
      " [  6945.26904297  24658.58398438 -16643.015625   ...,  11687.38183594\n",
      "   13803.171875   -19239.6015625 ]\n",
      " [ 11052.17578125  20529.7890625   -8923.32324219 ...,   4374.68798828\n",
      "    6410.59619141 -17396.87304688]\n",
      " [ -6236.92773438  39356.99609375 -31141.21289062 ...,  22067.84179688\n",
      "    7702.57470703 -25649.0390625 ]]\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "[ 32237.31054688      0.          21136.2578125   56514.21875\n",
      "   9930.24511719  14707.953125    41995.30859375  10659.04492188\n",
      "  61633.0234375   21312.58203125  20873.75585938  23360.53125         0.\n",
      "  28906.58007812      0.          25372.1796875   65424.453125\n",
      "  26426.16796875  10411.0546875   37091.1015625       0.          11528.0859375\n",
      "  33936.96875      1754.11914062  14549.28515625      0.           6249.76367188\n",
      "   5359.27929688  16303.12695312  16586.84570312  20069.66796875\n",
      "  59728.91796875  51978.91015625  48132.6875          0.           4455.21679688\n",
      "  59196.7421875   82653.53125     36573.4609375   80932.671875\n",
      "  60472.8984375   25512.4375      36595.51953125  13743.47753906\n",
      "  44850.56640625  51724.40625     33680.2109375   19738.23632812      0.\n",
      "  53362.0234375   68327.8828125   38391.7734375   19075.05078125\n",
      "   6317.7421875     427.2734375    3864.08007812  30864.65234375\n",
      "  19211.15820312  74182.7109375   35578.046875     4236.73535156\n",
      "  51716.328125    70083.90625     36198.41796875  60345.0625\n",
      "  36171.01953125  46440.546875        0.          41061.76171875\n",
      "  13181.63574219  32946.30078125  68043.5390625   83924.609375    15266.59375\n",
      "  39118.76171875  25453.625       52943.42578125  56589.8203125\n",
      "  65828.59375     30949.57617188  14087.51269531    812.46289062\n",
      "   7300.03125     19121.40234375  56594.8828125   67200.109375        0.\n",
      "  38681.5             0.          45482.0859375   10039.87304688\n",
      "  25933.50390625  38765.91015625  49603.7890625    3639.63476562\n",
      "  36955.50390625  58954.19140625  52346.0234375       0.          28355.4921875\n",
      "  10637.6328125   59732.9921875   23217.75976562  35331.39453125\n",
      "  47079.76953125   3428.16796875  78759.734375    17011.18359375      0.\n",
      "   7437.12109375  11487.32324219  21219.57421875   9436.8671875\n",
      "   1192.55859375  51675.84765625  11906.60351562  41039.12890625\n",
      "  21459.15234375  82205.59375     63267.0234375   65866.5078125\n",
      "  20251.3671875       0.          30457.75195312  50939.54296875\n",
      "  43739.55078125  16932.53515625  80465.734375     5244.23828125\n",
      "  11743.4921875    4239.36132812  12756.20214844  65910.53125\n",
      "    435.05273438   9835.76953125  29414.99804688  21033.82617188\n",
      "   4012.72851562  50169.9453125   79410.0078125   37976.546875\n",
      "  44576.14453125  67981.7890625   53862.7578125   18523.44726562\n",
      "  33256.98828125  20883.31835938  43898.1875      35210.86328125\n",
      "  27803.0078125 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-78372a248fd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentropy_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlrate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDROPOUT\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Optimization Finished!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # to visualize using TensorBoard (tensorboard --logdir=\"./graphs/convnet\" --port 6006)\n",
    "    writer = tf.summary.FileWriter('./graphs/convnet', sess.graph)\n",
    "\n",
    "    start_time = time.time()\n",
    "    n_batches = int(mnist.train.num_examples / BATCH_SIZE)\n",
    "\n",
    "    # Train the model\n",
    "    for index in range(n_batches * N_EPOCHS): # train the model n_epochs times\n",
    "        X_batch, Y_batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "        learning_rate = MIN_LR + (MAX_LR - MIN_LR) * math.exp(-index/DECAY_SPEED)\n",
    "        \n",
    "        if index % SKIP_STEP == 0:\n",
    "            loss_batch, accuracy_batch = sess.run([loss, accuracy], \n",
    "                                feed_dict={X: X_batch, Y:Y_batch, lrate: learning_rate, dropout: 1.0}) \n",
    "            print('Step {}: loss = {:5.1f}, accuracy = {:1.3f}'.format(index, loss_batch, accuracy_batch))\n",
    "    \n",
    "        sess.run(optimizer, feed_dict={X: X_batch, Y: Y_batch, lrate: learning_rate, dropout: DROPOUT})\n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time: {:.2f} seconds\".format(time.time() - start_time))\n",
    "    \n",
    "    # Test the model\n",
    "    X_batch, Y_batch = mnist.test.next_batch(BATCH_SIZE)\n",
    "    _, loss_batch, accuracy_batch = sess.run([optimizer, loss, accuracy], \n",
    "                                    feed_dict={X: mnist.test.images, Y: mnist.test.labels, lrate: learning_rate, dropout: DROPOUT}) \n",
    "    print(\"Accuracy = {:1.3f}; loss = {:1.3f}\".format(accuracy_batch, loss_batch))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
