{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to train a neural network model in order to read hand-written digits automatically. It uses the `Tensorflow` library, developed by Google.\n",
    "\n",
    "Although the notebook is divided into smaller steps, three main task will be of interest: network conception, optimization design and model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: module imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among necessary modules, there is of course Tensorflow; but also an utilitary for reading state-of-the-art data sets, like MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "# Alternative choice: from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: data recovering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Read in data using TF Learn's built in function to load MNIST data to the folder data/mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = read_data_sets(\"data\", one_hot=True, reshape=False, validation_size=0)\n",
    "# If alternative module import: mnist = input_data.read_data_sets(\"/data/mnist\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: parameter definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define paramaters for the model:\n",
    "- hidden layer depth (number of channel per convolutional and fully connected layer)\n",
    "- number of output classes\n",
    "- number of images per batch\n",
    "- number of epochs (one epoch = all images have been used for training)\n",
    "- decaying learning rate: fit the learning rate during training according to the convergence step (larger at the beginning, smaller at the end), the used formula is the following: min_lr + (max_lr-min_lr)*math.exp(-i/decay_speed), with i being the training iteration\n",
    "- dropout, *i.e.* percentage of nodes that are briefly removed during training process\n",
    "- printing frequency during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L_C1 = 32\n",
    "L_C2 = 64\n",
    "L_FC = 512\n",
    "N_CLASSES = 10\n",
    "\n",
    "BATCH_SIZE = 150\n",
    "N_EPOCHS = 5\n",
    "\n",
    "MAX_LR = 0.003\n",
    "MIN_LR = 0.0001\n",
    "DECAY_SPEED = 1000.0\n",
    "DROPOUT = 0.75\n",
    "\n",
    "SKIP_STEP = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: create placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Tensorflow, placeholders refer to variables that will be fed each time the model is run.\n",
    "\n",
    "Each image in the MNIST data is of shape 28*28*1 (greyscale) therefore, each image is represented with a 28*28*1 tensor; use None for shape so we can change the batch_size once we've built the tensor graph. The resulting output is a vector of `N_CLASSES` 0-1 values, the only '1' being the model prediction.\n",
    "\n",
    "As we work with a decaying learning rate, this quantity is managed within a placeholder. We'll be doing dropout for hidden layer so we'll need a placeholder for the dropout probability too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"data\"):\n",
    "    # Input X: 28x28 grayscale images, the first dimension (None) will index the images in the mini-batch\n",
    "    # If alternative module import: X = tf.placeholder(tf.float32, [None, 784], name=\"X\")\n",
    "    X = ...\n",
    "    # Output Y: vector of N_CLASSES values (either 0 or 1)\n",
    "    Y = ...\n",
    "# Variable learning rate\n",
    "lrate = ...\n",
    "# Dropout proportion\n",
    "dropout = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is composed of the following steps:\n",
    "\n",
    "conv -> relu -> pool -> conv -> relu -> pool -> fully connected -> softmax\n",
    "\n",
    "- conv: convolution between an input neuron and an image filter\n",
    "- relu (REctified Linear Unit): neuron activation function\n",
    "- pool: max pooling layer, that considers the maximal value in a n*n patch\n",
    "- fully connected: full connection between two consecutive neuron layer, concretized by a matrix multiplication\n",
    "- softmax: neuron activation function, associated with output\n",
    "\n",
    "They represent its structure, and may be showed within graph with `tensorboard` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first image convolution is applied to the input image: as a first parameter to optimize during procedure, numerical kernels are used to transform the image pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('conv1') as scope:\n",
    "    # If alternative module import, reshape the image to [BATCH_SIZE, 28, 28, 1]\n",
    "    # X = tf.reshape(X, shape=[-1, 28, 28, 1])\n",
    "    # Create kernel variable of dimension [5, 5, 1, L_C1] (initializer=tf.truncated_normal_initializer())\n",
    "    kernel = ...\n",
    "    # Create biases variable of dimension [L_C1] (initializer=tf.constant_initializer(0.0))\n",
    "    biases = ...\n",
    "    # Apply a convolution with tf.nn.conv2d, strides [1, 1, 1, 1], padding is 'SAME'\n",
    "    conv = ...\n",
    "    # Apply relu activation function (tf.nn.relu) on the sum of convolution output and biases\n",
    "    conv1 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output is of dimension BATCH_SIZE \\* 28 \\* 28 \\* L_C1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First pooling layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the dimensionality of the image, a pooling step is added: by considering the maximum value amongst neighboring pixels, we can simplify the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('pool1') as scope:\n",
    "    # Apply max pooling (tf.nn.pool) with ksize [1, 2, 2, 1], and strides [1, 2, 2, 1], padding 'SAME'    \n",
    "    pool1 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output is of dimension BATCH_SIZE \\* 14 \\* 14 \\* L_C1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('conv2') as scope:\n",
    "    # Create kernel variable of dimension [5, 5, L_C1, L_C2] (initializer=tf.truncated_normal_initializer())\n",
    "    kernel = ...\n",
    "    # Create biases variable of dimension [L_C2] (initializer=tf.constant_initializer(0.0))\n",
    "    biases = ...\n",
    "    # Apply a convolution with tf.nn.conv2d, strides [1, 1, 1, 1], padding is 'SAME'\n",
    "    conv = ...\n",
    "    # Apply relu activation function (tf.nn.relu) on the sum of convolution output and biases\n",
    "    conv2 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output is of dimension BATCH_SIZE \\* 14 \\* 14 \\* L_C2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('pool2') as scope:\n",
    "    # Apply max pooling (tf.nn.pool) with ksize [1, 2, 2, 1], and strides [1, 2, 2, 1], padding 'SAME'    \n",
    "    pool2 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output is of dimension BATCH_SIZE \\* 7 \\* 7 \\* L_C2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('fc') as scope:\n",
    "    input_features = 7 * 7 * L_C2\n",
    "    # Weights are of shape [7*7*L_C2, L_FC] (initializer=tf.truncated_normal_initializer())\n",
    "    w = ...\n",
    "    # Biases are of shape [L_FC] (initializer=tf.constant_initializer(0.0))\n",
    "    b = ...\n",
    "    # Reshape (tf.reshape) pool2 to 2-dimensional array (for applying matrix operations): BATCH_SIZE rows and 7*7*L_C2 columns\n",
    "    pool2 = ...\n",
    "    # Apply relu (tf.nn.relu) activation function on matmul of pool2 and weights, and add biases\n",
    "    fc = ...    \n",
    "    # Apply dropout (tf.nn.dropout) to the fully connected layer, by using the dropout parameter for drop proportion\n",
    "    fc = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point there remains to transform the previous layer into a layer of N_CLASSES channels, to express model outputs. This operation is represented by a new matrix multiplication, and a call to `softmax` as the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('softmax_linear') as scope:\n",
    "    # Get logits without softmax you need to create weights and biases\n",
    "    # Weights are variables of format [L_FC, N_CLASSES] (initializer=tf.truncated_normal_initializer())\n",
    "    w = ...\n",
    "    # Biases are variables of format [N_CLASSES] (initializer=tf.random_normal_initializer())\n",
    "    b = ...\n",
    "    # The model logit is given by the standard matrix operation (fc * w + b)\n",
    "    logits = ...\n",
    "    # Final model outputs are given by the logit transformation with the activation function (tf.nn.softmax)\n",
    "    Ypredict = ...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: loss function design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use cross-entropy loss function, normalised for batches of 100 images: `-sum(Y_i * log(Yi))`\n",
    "\n",
    "`TensorFlow` provides the `softmax_cross_entropy_with_logits` function to avoid numerical stability problems with log(0) (which is NaN).\n",
    "\n",
    "Furthermore the accuracy of the model is computed by comparing true Y values and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    # Cross-entropy between predicted (logits) and real (labels) values (tf.nn.softmax_cross_entropy_with_logits)\n",
    "    entropy = ...\n",
    "    # The model loss is the mean entropy over all observations (tf.reduce_mean)\n",
    "    loss = ...\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    # Accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "    # A correct prediction corresponds to equal output vectors (hint: find the indices for which the value is '1')\n",
    "    correct_prediction = ...\n",
    "    # The model accuracy is the mean over all prediction values (tf.reduce_mean)\n",
    "    # A cast operation is needed to express predictions as floating number (tf.float32)\n",
    "    accuracy = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Define training optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Adam optimizer with decaying learning rate to minimize cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimize loss with the help of Adam optimizer (tf.train.AdamOptimizer), do not forget to pass the learning rate\n",
    "optimizer = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final step: running the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Run the initialization of the variables\n",
    "    ...\n",
    "    # Create a graph summary\n",
    "    writer = tf.summary.FileWriter('./graphs/convnet', sess.graph)\n",
    "    # Compute the number of image batches\n",
    "    n_batches = int(mnist.train.num_examples / BATCH_SIZE)\n",
    "\n",
    "    # Train the model\n",
    "    start_time = time.time()\n",
    "    for index in range(n_batches * N_EPOCHS): # train the model n_epochs times\n",
    "        # Extract input and output images for current batch (mnist.train.next_batch)\n",
    "        X_batch, Y_batch = ...\n",
    "        # Compute the current learning rate (as a reminder we use a decaying rate)\n",
    "        learning_rate = MIN_LR + (MAX_LR - MIN_LR) * math.exp(-index/DECAY_SPEED)\n",
    "        # According to index value, print the current state of the model training\n",
    "        if index % SKIP_STEP == 0:\n",
    "            # Run the model without dropping out neurons (dropout=1.0)\n",
    "            loss_batch, accuracy_batch = ...\n",
    "            print('Step {}: loss = {:5.1f}, accuracy = {:1.3f}'.format(index, loss_batch, accuracy_batch))\n",
    "        # Train the model for the current index\n",
    "        ...\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time: {:.2f} seconds\".format(time.time() - start_time))\n",
    "    \n",
    "    # Test the model\n",
    "    # Run the model with test data (mnist.test.images, mnist.test.labels)\n",
    "    loss_test, accuracy_test = ...\n",
    "    print(\"Accuracy = {:1.3f}; loss = {:1.3f}\".format(accuracy_test, loss_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
